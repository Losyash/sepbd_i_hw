{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xydrmfnDr7KX"
      },
      "source": [
        "### Установка пакетов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezujwGoDr-on",
        "outputId": "6516254f-51e3-4612-9c03-17528b4a4abe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3CMXpjpptKn"
      },
      "source": [
        "### Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0bxIuW3BnoW3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.sql import SparkSession\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K0wrzBFOh6b"
      },
      "source": [
        "### Создание каталога data для хранения данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k6mJK8UWOgEZ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('data'):\n",
        "  os.makedirs('data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29kEYlPZp64Z"
      },
      "source": [
        "### Создание экземпляра SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IGshgYlPsS5r"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName('Homework_3_lab3').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Бинарная классификация (BinaryClassification)"
      ],
      "metadata": {
        "id": "F_vuNOz0odR3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-SD5MFn07MT"
      },
      "source": [
        "### Задание 1. Сгенерировать подходящие исходные данные для проведения обучения"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1. Определение функции генерации данных"
      ],
      "metadata": {
        "id": "RA7lRjAGq0N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_file(path, X, y):\n",
        "  lines = []\n",
        "\n",
        "  for i in range(len(y)):\n",
        "    features = []\n",
        "\n",
        "    for j in range(len(X[0])):\n",
        "      features.append(f\"{j+1}:{X[i][j]}\")\n",
        "\n",
        "    lines.append(f\"{y[i]} {' '.join(features)}\")\n",
        "\n",
        "  with open(path, \"w\") as file:\n",
        "    file.write('\\n'.join(lines))"
      ],
      "metadata": {
        "id": "I42QdeTmqytZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2. Генерация данных"
      ],
      "metadata": {
        "id": "eQbWA_taq9NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(\n",
        "  n_samples=1000,\n",
        "  n_features=15,\n",
        "  n_informative=12,\n",
        "  n_redundant=3,\n",
        "  n_clusters_per_class=2\n",
        ")\n",
        "\n",
        "to_file('data/classification_data', X, y)"
      ],
      "metadata": {
        "id": "pWiSAG0PoqOI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3. Чтение данных из файла"
      ],
      "metadata": {
        "id": "plz46mW2rDwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('libsvm').load('data/classification_data')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNvJFs0HrOoe",
        "outputId": "80f29e36-6027-4c4a-8f88-e96932a34dfb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(label=1.0, features=SparseVector(15, {0: -2.5421, 1: -0.2669, 2: -0.1468, 3: -0.2975, 4: 2.617, 5: 0.1723, 6: -3.35, 7: 3.2991, 8: -0.7271, 9: 0.7871, 10: -0.4746, 11: 1.3936, 12: -0.2733, 13: 6.2807, 14: 1.3179}))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2. Провести бинарную классификацию с помощью pyspark.ml / pyspark.mllib"
      ],
      "metadata": {
        "id": "LiVUFLbniCrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_prepared = df.rdd.map(lambda row: LabeledPoint(row.label, list(row.features))).collect()\n",
        "df_prepared = spark.sparkContext.parallelize(df_prepared)"
      ],
      "metadata": {
        "id": "TlOvd0dasm11"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = df_prepared.randomSplit([0.8, 0.2])"
      ],
      "metadata": {
        "id": "sY7N9m35uOxb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegressionWithLBFGS.train(df_train)"
      ],
      "metadata": {
        "id": "N-vFdi_4uZ0l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = df_test.map(lambda row: (float(model.predict(row.features)), row.label))"
      ],
      "metadata": {
        "id": "EJft7ikBufwx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 3. Провести расчет метрик"
      ],
      "metadata": {
        "id": "Nd98rL38iGMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = BinaryClassificationMetrics(predictions)\n",
        "(metrics.areaUnderPR, metrics.areaUnderROC)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kYmu1pZu4K7",
        "outputId": "e3804ba0-6421-40e9-efa7-a464a5422e8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7746040723981901, 0.7974472807991121)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 4. Сохранить полученные значения метрик в файл"
      ],
      "metadata": {
        "id": "NrtUDb4ziKqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_list = [ metrics.areaUnderPR, metrics.areaUnderROC ]\n",
        "\n",
        "with open('data/classification_data_r_1', 'w') as file:\n",
        "  file.write(str(metrics_list))"
      ],
      "metadata": {
        "id": "49lJlkalz7QT"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}