{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Установка пакетов"
      ],
      "metadata": {
        "id": "vxZYORSgPBQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "ePKvY0DIPDXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cefe91d-c9ff-49d5-b193-c0cedfeeafb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Импорт библиотек"
      ],
      "metadata": {
        "id": "cEBdCcCdPGML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0bxIuW3BnoW3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_timestamp, col, current_timestamp, from_unixtime, window, avg, max\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
        "import random\n",
        "import time\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K0wrzBFOh6b"
      },
      "source": [
        "### Создание каталога data для хранения данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k6mJK8UWOgEZ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('data/input'):\n",
        "  os.makedirs('data/input')\n",
        "\n",
        "if not os.path.exists('data/output'):\n",
        "  os.makedirs('data/output')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29kEYlPZp64Z"
      },
      "source": [
        "### Создание экземпляра SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IGshgYlPsS5r"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession\\\n",
        "  .builder \\\n",
        "  .appName('Homework_5_lab3') \\\n",
        "  .config('spark.sql.streaming.schemaInference', 'true') \\\n",
        "  .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Применение Sliding Window"
      ],
      "metadata": {
        "id": "0IPo2wI4ABzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1. Подготовить подходящие данные или воспользоваться продемонстрированным на паре генератором"
      ],
      "metadata": {
        "id": "rwjfAAEF4ipo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in range(random.randint(5, 20)):\n",
        "  with open(f'data/input/{chunk}.json', 'w') as file:\n",
        "    for _ in range(random.randint(10, 20)):\n",
        "      file.write(json.dumps({\n",
        "        'CreatedAt': int(time.time()) + 4 * chunk,\n",
        "        'Price': random.randint(1000, 10_000) / 100,\n",
        "        'Quantity': random.randint(100, 2000),\n",
        "        'Type': random.choice([ 'type1', 'type2', 'type3' ])\n",
        "      }))\n",
        "\n",
        "      file.write('\\n')"
      ],
      "metadata": {
        "id": "O2nJIozUg5vN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "  StructField('CreatedAt', StringType()),\n",
        "  StructField('Price', DoubleType()),\n",
        "  StructField('Quantity', IntegerType()),\n",
        "  StructField('Type', StringType())\n",
        "])"
      ],
      "metadata": {
        "id": "M5lx2T7nkqKJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2. Открыть на основе исходных json файлов потоковый датафрейм"
      ],
      "metadata": {
        "id": "uXw3kOahmSC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.readStream \\\n",
        "  .format('json') \\\n",
        "  .option('path', 'data/input') \\\n",
        "  .option('maxFilesPerTrigger', 1) \\\n",
        "  .schema(schema) \\\n",
        "  .load()"
      ],
      "metadata": {
        "id": "UHaPPVT50gnG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr4FFipTk9aQ",
        "outputId": "f0a80898-94e9-4bb6-ecbc-fbb90688ab57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CreatedAt: string (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Type: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prepared = df.withColumn(\"CreatedAt\", to_timestamp(from_unixtime(col(\"CreatedAt\"))))"
      ],
      "metadata": {
        "id": "gt41R0Jwldxg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_prepared.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpYkna0Elzmt",
        "outputId": "470adc87-cdfe-4970-f14e-3ecb3405a334"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CreatedAt: timestamp (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Type: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 3. Написать произвольный запрос агрегации, используя функцию tumbling window"
      ],
      "metadata": {
        "id": "FNOZl5FXgov7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_aggregated = df_prepared \\\n",
        "  .groupBy(window(col('CreatedAt'), '10 seconds', '5 second')) \\\n",
        "  .agg(\n",
        "    avg('Price').alias('price_avg'),\n",
        "    max('Quantity').alias('quantity_max')\n",
        "  )"
      ],
      "metadata": {
        "id": "So_IZMTUrYbQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result = df_aggregated.select(\"window.start\", \"window.end\", \"price_avg\", \"quantity_max\")"
      ],
      "metadata": {
        "id": "JMgmGUD1rzY9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.printSchema()"
      ],
      "metadata": {
        "id": "4h7q6zHRsAwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f0738e-9cc4-45a4-8242-c2ce19d60ccf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- start: timestamp (nullable = true)\n",
            " |-- end: timestamp (nullable = true)\n",
            " |-- price_avg: double (nullable = true)\n",
            " |-- quantity_max: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stream = df_result.writeStream \\\n",
        "  .format('memory') \\\n",
        "  .outputMode('update') \\\n",
        "  .option('queryName', 'query') \\\n",
        "  .option('checkpointLocation', 'data/check-dir') \\\n",
        "  .trigger(processingTime='5 seconds') \\\n",
        "  .start()"
      ],
      "metadata": {
        "id": "ojPiqzcAsLpi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM query\").show()"
      ],
      "metadata": {
        "id": "1fuaZ-Hiuthd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1633154-44b1-4c84-9ebf-78b9a3c4533b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+------------------+------------+\n",
            "|              start|                end|         price_avg|quantity_max|\n",
            "+-------------------+-------------------+------------------+------------+\n",
            "|2024-12-27 13:17:40|2024-12-27 13:17:50|49.672999999999995|        1972|\n",
            "|2024-12-27 13:17:40|2024-12-27 13:17:50| 51.38633333333333|        1972|\n",
            "+-------------------+-------------------+------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 4. Сохранить получаемые каждые X секунд исходные файлы в json"
      ],
      "metadata": {
        "id": "2su8tutRgpOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "currentBatch = 0\n",
        "\n",
        "while not stream.awaitTermination(30):\n",
        "  lastProgress = stream.lastProgress\n",
        "  batchId = lastProgress['batchId']\n",
        "\n",
        "  if lastProgress['numInputRows'] == 0:\n",
        "    spark.sql(\"SELECT * FROM query\").write.json(f\"data/output/{batchId}.json\")\n",
        "    stream.stop()"
      ],
      "metadata": {
        "id": "IK5vW7w4uy4Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spark.stop()"
      ],
      "metadata": {
        "id": "bTluD8G1u061"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}